{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Iteration\n",
    "Now that I am done scraping (please dear god no more I dont want to for the time being), I'm going to do a clustering thingy thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T03:58:06.464851Z",
     "start_time": "2018-08-17T03:58:06.453639Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T03:09:53.141302Z",
     "start_time": "2018-08-17T03:09:52.431958Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The last leaf clings to the bough. Just one le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  The last leaf clings to the bough. Just one le..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in my dataset\n",
    "\n",
    "song_df = pd.DataFrame(pd.read_pickle('third_round_results_lyrics.pkl')).reset_index()\n",
    "song_df.drop('index',axis = 1, inplace = True, errors = 'ignore')\n",
    "lyrics = song_df.drop(['artist', 'song'], axis = 1, errors = 'ignore')\n",
    "lyrics.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T03:25:05.353213Z",
     "start_time": "2018-08-17T03:25:04.757908Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyrics.lyrics = lyrics.lyrics.apply(', '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing is that when you remove baby or like or love or honey, you only get 2 topics.... WHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:02:56.177343Z",
     "start_time": "2018-08-17T04:02:56.171694Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"na\", 'instrumental', 'yeah', 'ha', 'ok', 'babe', 'oo'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:03:03.098660Z",
     "start_time": "2018-08-17T04:02:59.030666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words=my_stop_words, \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "cv_data = count_vectorizer.fit_transform(list(lyrics.lyrics.values))\n",
    "tfidf_data = tfidf_vectorizer.fit_transform(list(lyrics.lyrics.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:03:03.194686Z",
     "start_time": "2018-08-17T04:03:03.184253Z"
    },
    "code_folding": [
     0,
     1
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:05:25.392721Z",
     "start_time": "2018-08-17T04:05:23.861944Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "love conquer, don cause, baby bow, know darling, oh honey\n",
      "\n",
      "Topic  1\n",
      "love conquer, love remains, oh vivid, oh honey, just request\n",
      "\n",
      "Topic  2\n",
      "oh honey, oh vivid, baby bow, come da, yes hold\n",
      "\n",
      "Topic  3\n",
      "baby bow, baby coast, yes hold, zoooom, come dim\n",
      "\n",
      "Topic  4\n",
      "lady ll, lady takin, natural selection, naughty, yes hold\n",
      "\n",
      "Topic  5\n",
      "lady ll, don cause, lady takin, ll discover, natural selection\n",
      "\n",
      "Topic  6\n",
      "ll discover, come da, awake hours, heart falling, ll send\n",
      "\n",
      "Topic  7\n",
      "natural selection, naughty, ll discover, door bibles, sad crying\n",
      "\n",
      "Topic  8\n",
      "intoxicatin, like capital, feel cries, ll discover, come da\n",
      "\n",
      "Topic  9\n",
      "yes hold, zoooom, don cause, let feel, ll discover\n",
      "\n",
      "Topic  10\n",
      "let feel, come da, want light, like capital, got burning\n",
      "\n",
      "Topic  11\n",
      "want light, got burning, ll discover, know darling, need lonely\n",
      "\n",
      "Topic  12\n",
      "got burning, ve long, ve spiritual, let feel, hold note\n",
      "\n",
      "Topic  13\n",
      "come da, ve long, feel cries, come feelings, want light\n",
      "\n",
      "Topic  14\n",
      "want light, awake hours, gonna everybody, life hell, war dreams\n",
      "\n",
      "Topic  15\n",
      "know darling, gonna everybody, come da, just delicate, tell needs\n",
      "\n",
      "Topic  16\n",
      "gonna everybody, ll discover, tell needs, let feel, make forget\n",
      "\n",
      "Topic  17\n",
      "rucker sweatin, rules learn, life hell, girl didn, know darling\n",
      "\n",
      "Topic  18\n",
      "feel cries, hold note, make forget, girl didn, realest\n",
      "\n",
      "Topic  19\n",
      "way hustling, hey mind, rucker sweatin, rules learn, long oh\n",
      "\n",
      "Topic  20\n",
      "say hopeless, rucker sweatin, let feel, rules learn, know darling\n",
      "\n",
      "Topic  21\n",
      "rucker sweatin, feel cries, rules learn, make forget, hair skin\n",
      "\n",
      "Topic  22\n",
      "way hustling, know darling, feel cries, let feel, long oh\n",
      "\n",
      "Topic  23\n",
      "good feel, girl didn, say hopeless, life hell, man half\n",
      "\n",
      "Topic  24\n",
      "took, girl didn, awake hours, little late, hold note\n",
      "\n",
      "Topic  25\n",
      "took, life hell, time help, got burning, hold note\n",
      "\n",
      "Topic  26\n",
      "heart falling, hair skin, hair tryin, tell needs, let feel\n",
      "\n",
      "Topic  27\n",
      "hair skin, hair tryin, hold note, make forget, hey mind\n",
      "\n",
      "Topic  28\n",
      "good feel, heart falling, life hell, gonna everybody, hair skin\n",
      "\n",
      "Topic  29\n",
      "wife just, took, wildcats, little late, realest\n",
      "\n",
      "Topic  30\n",
      "heart falling, stopped flies, way hustling, dance mirror, say hopeless\n",
      "\n",
      "Topic  31\n",
      "tell needs, dance mirror, gone live, night know, care nothin\n",
      "\n",
      "Topic  32\n",
      "wife just, dance mirror, wildcats, gone live, rocking millie\n",
      "\n",
      "Topic  33\n",
      "hold note, little late, tell needs, holding arms, bit needing\n",
      "\n",
      "Topic  34\n",
      "need lonely, ve long, door bibles, life hell, hey mind\n",
      "\n",
      "Topic  35\n",
      "door bibles, door blocked, hold note, wife just, dance mirror\n",
      "\n",
      "Topic  36\n",
      "won long, man half, door bibles, heart falling, long oh\n",
      "\n",
      "Topic  37\n",
      "wife just, took, tell needs, just delicate, stopped flies\n",
      "\n",
      "Topic  38\n",
      "door bibles, love remains, door blocked, hey mind, tell needs\n",
      "\n",
      "Topic  39\n",
      "eyes feel, good feel, need lonely, hey mind, wanna sing\n",
      "\n",
      "Topic  40\n",
      "gotta just, walking away, love remains, just delicate, right inch\n",
      "\n",
      "Topic  41\n",
      "standing takes, hey mind, wonder breakaway, better half, good feel\n",
      "\n",
      "Topic  42\n",
      "standing takes, gotta just, wanna sing, hey mind, cause decide\n",
      "\n",
      "Topic  43\n",
      "cool round, talked piece, loves chills, coolest, dance mirror\n",
      "\n",
      "Topic  44\n",
      "walking away, night know, way hustling, good feel, oh vivid\n",
      "\n",
      "Topic  45\n",
      "wartime novelty, talked piece, man half, loves chills, stopped flies\n",
      "\n",
      "Topic  46\n",
      "low long, sweetness charms, making crazy, cause decide, wood just\n",
      "\n",
      "Topic  47\n",
      "walking away, dance mirror, wartime novelty, need lonely, dream holding\n",
      "\n",
      "Topic  48\n",
      "cool round, open showed, walking away, fool plays, thing want\n",
      "\n",
      "Topic  49\n",
      "question know, wonder breakaway, feeling got, little late, won long\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We do one of these for you (lsa_tfidf)\n",
    "# Make the other two\n",
    "n_comp = 50\n",
    "lsa_tfidf = TruncatedSVD(n_components=n_comp)\n",
    "\n",
    "lsa_tfidf_data = lsa_tfidf.fit_transform(tfidf_data)\n",
    "\n",
    "display_topics(lsa_tfidf,count_vectorizer.get_feature_names(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:05:34.896231Z",
     "start_time": "2018-08-17T04:05:34.071290Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "love conquer, don cause, baby bow, know darling, oh honey\n",
      "\n",
      "Topic  1\n",
      "love conquer, love remains, oh vivid, oh honey, just request\n",
      "\n",
      "Topic  2\n",
      "oh honey, oh vivid, baby bow, come da, yes hold\n",
      "\n",
      "Topic  3\n",
      "baby bow, baby coast, yes hold, zoooom, come dim\n",
      "\n",
      "Topic  4\n",
      "lady ll, lady takin, natural selection, naughty, yes hold\n",
      "\n",
      "Topic  5\n",
      "lady ll, don cause, lady takin, ll discover, natural selection\n",
      "\n",
      "Topic  6\n",
      "ll discover, come da, awake hours, ll send, heart falling\n",
      "\n",
      "Topic  7\n",
      "natural selection, naughty, ll discover, door bibles, sad crying\n",
      "\n",
      "Topic  8\n",
      "intoxicatin, like capital, heart falling, gonna everybody, awake hours\n",
      "\n",
      "Topic  9\n",
      "yes hold, zoooom, don cause, let feel, ll discover\n",
      "\n",
      "Topic  10\n",
      "let feel, come da, want light, like capital, got burning\n",
      "\n",
      "Topic  11\n",
      "want light, ll discover, got burning, know darling, need lonely\n",
      "\n",
      "Topic  12\n",
      "got burning, ve long, ve spiritual, let feel, hold note\n",
      "\n",
      "Topic  13\n",
      "come da, ve long, feel cries, come feelings, want light\n",
      "\n",
      "Topic  14\n",
      "want light, awake hours, gonna everybody, life hell, war dreams\n",
      "\n",
      "Topic  15\n",
      "know darling, gonna everybody, come da, just delicate, tell needs\n",
      "\n",
      "Topic  16\n",
      "rucker sweatin, rules learn, heart falling, girl didn, need lonely\n",
      "\n",
      "Topic  17\n",
      "rucker sweatin, rules learn, gonna everybody, know darling, let feel\n",
      "\n",
      "Topic  18\n",
      "feel cries, little late, make forget, heart falling, bit needing\n",
      "\n",
      "Topic  19\n",
      "way hustling, hair skin, hey mind, ll discover, come da\n",
      "\n",
      "Topic  20\n",
      "make forget, feel cries, rucker sweatin, hair skin, hair tryin\n",
      "\n",
      "Topic  21\n",
      "say hopeless, rucker sweatin, rules learn, hold note, time help\n",
      "\n",
      "Topic  22\n",
      "feel cries, hey mind, way hustling, eyes feel, little late\n",
      "\n",
      "Topic  23\n",
      "good feel, life hell, say hopeless, world straight, good season\n",
      "\n",
      "Topic  24\n",
      "took, dance mirror, girl didn, night know, wanna sing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We do one of these for you (lsa_tfidf)\n",
    "# Make the other two\n",
    "n_comp = 25\n",
    "lsa_tfidf = TruncatedSVD(n_components=n_comp)\n",
    "\n",
    "lsa_tfidf_data = lsa_tfidf.fit_transform(tfidf_data)\n",
    "\n",
    "display_topics(lsa_tfidf,count_vectorizer.get_feature_names(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T04:03:44.486838Z",
     "start_time": "2018-08-17T04:03:43.811228Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "love, oh, baby, don, know, just, like, got, come, let\n",
      "\n",
      "Topic  1\n",
      "la, la la, ohh, la ohh, nigga, nothin, ohh la, nigga nothin, yo, like\n",
      "\n",
      "Topic  2\n",
      "like, don, rock, know, party, party like, like rock, got, make, just\n",
      "\n",
      "Topic  3\n",
      "jump, jump jump, oh, like, party, rock, oh oh, party like, like rock, come\n",
      "\n",
      "Topic  4\n",
      "baby, come, let, ding, come baby, baby baby, baby come, ding ding, ll, bounce\n",
      "\n",
      "Topic  5\n",
      "come, like, baby, oh, party, party like, oh oh, like rock, rock, star\n",
      "\n",
      "Topic  6\n",
      "love, jump, baby, jump jump, love love, baby baby, party, party like, come, like\n",
      "\n",
      "Topic  7\n",
      "walk, walk walk, northside, southside, westside, southside walk, westside walk, walk eastside, northside walk, eastside\n",
      "\n",
      "Topic  8\n",
      "step, wit, step step, jiggy, jiggy wit, gon, gon step, don, wit jiggy, step gon\n",
      "\n",
      "Topic  9\n",
      "don, stop, don stop, rock, party like, stop don, party, force, stop til, force don\n",
      "\n",
      "Topic  10\n",
      "cool, cool cool, round, said, chill, ve, cool said, chill cool, round round, don\n",
      "\n",
      "Topic  11\n",
      "make, dance, duh, hot, rhythm, let, da, come, cool, rhythm rhythm\n",
      "\n",
      "Topic  12\n",
      "doo, doo doo, say, baby, tight, make, skin, skin tight, let, hey\n",
      "\n",
      "Topic  13\n",
      "say, ll, duh, come, know, da, time, let, say say, right\n",
      "\n",
      "Topic  14\n",
      "duh, da, doo, duh duh, da duh, doo doo, boys, whoa, got, let\n",
      "\n",
      "Topic  15\n",
      "doo, come, doo doo, ll, let, tight, skin, skin tight, gotta, tonight\n",
      "\n",
      "Topic  16\n",
      "round, round round, just, corner, round corner, corner round, push, baby, let, ll\n",
      "\n",
      "Topic  17\n",
      "like, let, gimme, gotta, like like, gimme gimme, want, wanna, ba, doop\n",
      "\n",
      "Topic  18\n",
      "say, hot, round, got, say say, come, hot hot, like, round round, doo\n",
      "\n",
      "Topic  19\n",
      "push, let, just, ah, feeling, got, push feeling, ll, hot, push push\n"
     ]
    }
   ],
   "source": [
    "# This displays the topics for using lsa on TF-IDF\n",
    "# Get the top 10 for lsa using CountVecotizer\n",
    "# Get the top 10 for NMF using CountVectorizer\n",
    "\n",
    "n_comp = 20\n",
    "lsa_cv = TruncatedSVD(n_components=n_comp)\n",
    "\n",
    "lsa_cv_data = lsa_cv.fit_transform(cv_data)\n",
    "\n",
    "display_topics(lsa_cv,count_vectorizer.get_feature_names(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
